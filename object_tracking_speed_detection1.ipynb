{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TharunShreeB/object-detection-speed-tracking-/blob/main/object_tracking_speed_detection1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pCBN-dI3Ko_",
        "outputId": "f2a186e0-07de-4297-fff1-cf5ed8689b1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 17410, done.\u001b[K\n",
            "remote: Counting objects: 100% (84/84), done.\u001b[K\n",
            "remote: Compressing objects: 100% (62/62), done.\u001b[K\n",
            "remote: Total 17410 (delta 63), reused 22 (delta 22), pack-reused 17326 (from 3)\u001b[K\n",
            "Receiving objects: 100% (17410/17410), 16.30 MiB | 17.24 MiB/s, done.\n",
            "Resolving deltas: 100% (11934/11934), done.\n",
            "/content/yolov5\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m853.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# STEP 1: Setup YOLOv5 and dependencies\n",
        "!git clone https://github.com/ultralytics/yolov5.git\n",
        "%cd yolov5\n",
        "!pip install -q opencv-python matplotlib seaborn tqdm\n",
        "!pip install -q torch torchvision\n",
        "\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import time\n",
        "import uuid\n",
        "from IPython.display import display, clear_output\n",
        "from google.colab.patches import cv2_imshow\n",
        "from google.colab import files\n",
        "\n",
        "# STEP 2: Load YOLOv5 model\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', force_reload=True)\n",
        "\n",
        "# STEP 3: Upload video file\n",
        "uploaded = files.upload()\n",
        "video_path = next(iter(uploaded))\n",
        "\n",
        "# STEP 4: Constants & video setup\n",
        "PIXEL_TO_METER = 0.5  # <-- Calibrate this properly!\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "FPS = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "vehicle_positions = {}  # {vehicle_id: (x, y)}\n",
        "\n",
        "out = cv2.VideoWriter('output.avi',\n",
        "                      cv2.VideoWriter_fourcc(*'XVID'),\n",
        "                      FPS,\n",
        "                      (int(cap.get(3)), int(cap.get(4))))\n",
        "\n",
        "frame_count = 0\n",
        "\n",
        "# STEP 5: Helper function for ID tracking\n",
        "def assign_ids(prev_positions, curr_positions, max_distance=50):\n",
        "    new_tracking = {}\n",
        "    used = set()\n",
        "\n",
        "    for prev_id, (px, py) in prev_positions.items():\n",
        "        closest_id = None\n",
        "        closest_dist = max_distance\n",
        "        for curr_id, (cx, cy) in curr_positions.items():\n",
        "            if curr_id in used:\n",
        "                continue\n",
        "            dist = np.linalg.norm(np.array([cx, cy]) - np.array([px, py]))\n",
        "            if dist < closest_dist:\n",
        "                closest_id = curr_id\n",
        "                closest_dist = dist\n",
        "\n",
        "        if closest_id is not None:\n",
        "            new_tracking[closest_id] = (curr_positions[closest_id], prev_id)\n",
        "            used.add(closest_id)\n",
        "\n",
        "    # Assign new IDs to unmatched detections\n",
        "    for curr_id, (cx, cy) in curr_positions.items():\n",
        "        if curr_id not in used:\n",
        "            new_tracking[curr_id] = ((cx, cy), str(uuid.uuid4())[:8])\n",
        "\n",
        "    # Return: {vehicle_id: position}\n",
        "    return {vehicle_id: position for (_, (position, vehicle_id)) in new_tracking.items()}\n",
        "\n",
        "# STEP 6: Main loop for detection and speed estimation\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    frame_count += 1\n",
        "    results = model(frame)\n",
        "    detections = results.xyxy[0].cpu().numpy()\n",
        "\n",
        "    current_raw = {}\n",
        "\n",
        "    for det in detections:\n",
        "        x1, y1, x2, y2, conf, cls = det\n",
        "        if int(cls) != 2:  # Only detect class 2 = car\n",
        "            continue\n",
        "\n",
        "        cx = int((x1 + x2) / 2)\n",
        "        cy = int((y1 + y2) / 2)\n",
        "        current_raw[f\"{cx}-{cy}\"] = (cx, cy)\n",
        "\n",
        "    # Match vehicles across frames\n",
        "    tracked_positions = assign_ids(vehicle_positions, current_raw)\n",
        "\n",
        "    # Draw boxes and display speeds\n",
        "    for vid, (cx, cy) in tracked_positions.items():\n",
        "        for det in detections:\n",
        "            x1, y1, x2, y2, conf, cls = det\n",
        "            if int(cls) != 2:\n",
        "                continue\n",
        "            box_cx = int((x1 + x2) / 2)\n",
        "            box_cy = int((y1 + y2) / 2)\n",
        "            if abs(box_cx - cx) < 5 and abs(box_cy - cy) < 5:\n",
        "                break\n",
        "\n",
        "        if vid in vehicle_positions:\n",
        "            px, py = vehicle_positions[vid]\n",
        "            dist_pixels = np.sqrt((cx - px) ** 2 + (cy - py) ** 2)\n",
        "            speed_mps = dist_pixels * PIXEL_TO_METER * FPS\n",
        "            speed_kmph = speed_mps * 3.6\n",
        "        else:\n",
        "            speed_kmph = 0\n",
        "\n",
        "        cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
        "        cv2.putText(frame, f\"{int(speed_kmph)} km/h\", (int(x1), int(y1) - 10),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)\n",
        "\n",
        "    vehicle_positions = tracked_positions\n",
        "    out.write(frame)\n",
        "\n",
        "    if frame_count % 10 == 0:\n",
        "        clear_output(wait=True)\n",
        "        cv2_imshow(frame)\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "\n",
        "# STEP 7: Download output\n",
        "files.download('output.avi')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjxUCoj53Lma"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+XG5jja7L5y6/K6B9B0q/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}